{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed677a2",
   "metadata": {},
   "source": [
    "# 스크래핑 복습 (part.3 basic 1 ~ basic 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색하고 싶은 단어 입력하기\n",
    "word = 'happiness'\n",
    "\n",
    "# 불러오려는 url 입력하기 \n",
    "# 디폴트 url에 string 타입의 word 변수를 합쳐서 url 변수 생성\n",
    "url = 'https://alldic.daum.net/search.do?q=' + word\n",
    "\n",
    "# urlopen 함수를 통해 web 변수를 생성\n",
    "web = urlopen(url)   # urlopen(url).read().decode('utf-8')\n",
    "\n",
    "#print(web) #뭉텅이로 가져오게 됨.\n",
    "\n",
    "# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱\n",
    "web_page = BeautifulSoup(web, 'html.parser') #html 방식으로 해석해줘~ 라는 의미\n",
    "\n",
    "print(web_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = web_page.find_all('span', {'class': 'txt_emph1'}) # 찾다\n",
    "print(box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0492a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "box1[0].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81be9f",
   "metadata": {},
   "source": [
    "영화정보 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  'https://movie.naver.com/movie/bi/mi/basic.naver?code=208077'\n",
    "web = urlopen(url)\n",
    "web_page = BeautifulSoup(web, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe416c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = web_page.find('h3',{'class':'h_movie'}).find('a')\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed412004",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://movie.naver.com/movie/bi/mi/detail.naver?code=208077' # 영화에 대한 배우/제작진 정보가 담긴 페이지\n",
    "web = urlopen(url)\n",
    "web_page = BeautifulSoup(web, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "director = web_page.find('div', {'class':'dir_product'}).find('a', {'class':'k_name'}) \n",
    "print('Director:', end =\" \")\n",
    "print(director.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b67135",
   "metadata": {},
   "source": [
    "신문기사 출력하고 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.seattletimes.com/business/real-estate/zillows-zestimate-overvalued-a-washington-home-by-700-percent-in-a-case-of-algorithms-gone-wrong/'\n",
    "web = urlopen(url)\n",
    "source = BeautifulSoup(web, 'html.parser')\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b57c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('seattletimes_text.txt', 'w', encoding = 'utf-8') as f:\n",
    "\n",
    "    times = source.find('div', {'class': 'article-content'})\n",
    "    article = times.find_all('p')\n",
    "    for content in article:\n",
    "        print(content.get_text())\n",
    "        f.write(content.get_text()+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57aae0",
   "metadata": {},
   "source": [
    "문장 토큰화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79f369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 전처리하고자 하는 문장을 String 변수로 저장한다\n",
    "sentence = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'\n",
    "\n",
    "# 각 문장을 토큰화한 결과를 출력한다\n",
    "nltk.word_tokenize(sentence)  # 문장을 토큰화해 출력한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c821e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 품사 태깅 하기 pos tagging\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 모듈에서 Stopwords를 직접 불러온다\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 영어의 stopwords를 불러와 변수에 저장한다\n",
    "stopWords = stopwords.words('english') # 지원 언어 목록 : stopwords.fileids()\n",
    "\n",
    "# How many stop words\n",
    "print(len(stopWords))\n",
    "print()\n",
    "\n",
    "# stop words 출력\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(',')\n",
    "stop_words.append('.')\n",
    "\n",
    "result = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        result.append(token)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe24258",
   "metadata": {},
   "source": [
    "Lemmatization : 어근에 기반한 기본 사전형인 lemma 찾아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62dc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"swam\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de56e5",
   "metadata": {},
   "source": [
    "# news article scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup # HTTP Response -> HTML \n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111e990",
   "metadata": {},
   "source": [
    "여러 뉴스 데이터 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://search.naver.com/search.naver?where=news&query=홈트&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2018.01.01&de=2018.01.31&start=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_dates(year, month):\n",
    "    if month < 10:\n",
    "        ds = str(year) +\".0\" + str(month) + \".01\"\n",
    "        de = str(year) +\".0\" + str(month) + \".31\" \n",
    "    else:\n",
    "        ds = str(year) +\".\" + str(month) + \".01\"\n",
    "        de = str(year) +\".\" + str(month) + \".31\"\n",
    "    return ds, de\n",
    "x1, x2 = making_dates(2018, 10)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c51d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#https://search.naver.com/search.naver?where=news&query=홈트&\n",
    "#pd=3&\n",
    "#ds=2018.01.01&\n",
    "#de=2018.01.31&\n",
    "#start=11\n",
    "\n",
    "def yearly_df(year, query = '홈트'):\n",
    "\n",
    "    month_counter = 1\n",
    "\n",
    "    while month_counter < 13:\n",
    "\n",
    "        ds, de = making_dates(year, month_counter)\n",
    "        pages = ['01', '11', '21']\n",
    "\n",
    "        for page in pages:\n",
    "            url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query='+query+'&sort=0&photo=0&field=0&pd=3&ds='+ds+'&de='+de+'&cluster_rank=15&mynews=0&office_type=0&office_section_code=0&news_office_checked=&start='+page\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "            # 1) 네이버 뉴스만 추려내기\n",
    "            urls_list = []\n",
    "            for urls in source.find_all('a', {'class': 'info'}):\n",
    "                if urls.attrs['href'].startswith('https://news.naver.com/'):\n",
    "                    urls_list.append(urls.attrs['href'])\n",
    "\n",
    "            titles = []\n",
    "            dates = []\n",
    "            articles = []\n",
    "            press_companies = []\n",
    "            article_urls = []\n",
    "            error_urls = []\n",
    "\n",
    "            for url in urls_list:\n",
    "                try:\n",
    "                    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "                    web_news = requests.get(url, headers=headers).content\n",
    "                    source_news = BeautifulSoup(web_news, 'html.parser')\n",
    "\n",
    "                    # 2) 기사 제목\n",
    "                    title = source_news.find('h3', {'id' : 'articleTitle'}).get_text()\n",
    "                    titles.append(title)\n",
    "                    print('Processing article : {}'.format(title))\n",
    "\n",
    "                    # 3) 기사 날짜\n",
    "                    date = source_news.find('span', {'class' : 't11'}).get_text()\n",
    "                    dates.append(date)\n",
    "\n",
    "                    # 4) 기사 본문\n",
    "                    article = source_news.find('div', {'id' : 'articleBodyContents'}).get_text()\n",
    "                    article = article.replace(\"\\n\", \"\")\n",
    "                    article = article.replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
    "                    article = article.replace(\"동영상 뉴스       \", \"\")\n",
    "                    article = article.replace(\"동영상 뉴스\", \"\")\n",
    "                    article = article.strip()\n",
    "                    articles.append(article)\n",
    "\n",
    "                    # 5) 기사 URL \n",
    "                    article_urls.append(url)\n",
    "\n",
    "                    # 6) 기사 발행 언론사\n",
    "                    press_company = source_news.find('address', {'class' : 'address_cp'}).find('a').get_text()\n",
    "                    press_companies.append(press_company)\n",
    "                except:\n",
    "                    print('*** 다음 링크의 뉴스를 크롤링하는 중 에러가 발생했습니다 : {}'.format(url))\n",
    "                    error_urls.append(url)\n",
    "            \n",
    "        month_counter += 1\n",
    "            \n",
    "    article_df = pd.DataFrame({'Title':titles,\n",
    "                               'Date':dates, \n",
    "                               'Article':articles, \n",
    "                               'URL':article_urls, \n",
    "                               'PressCompany':press_companies})\n",
    "\n",
    "        \n",
    "\n",
    "    article_df.to_excel('result_{}.xlsx'.format(year), index=False, encoding='utf-8')\n",
    "    article_df.head(10)\n",
    "    \n",
    "yearly_df(2018, '홈트')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ccc8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b6170a6",
   "metadata": {},
   "source": [
    "# 워드클라우드 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import re #정규표현식 사용\n",
    "\n",
    "import nltk #text processing\n",
    "from konlpy.tag import Okt #tag 안에 여러가지 형태소 분석기\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0045c",
   "metadata": {},
   "source": [
    "## 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27827601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# '홈트레이닝' 연도별 뉴스 기사 하나의 df로 합치기\n",
    "yrs = ['18','19','20','21']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for yr in yrs:\n",
    "    temp = pd.read_excel('홈트레이닝news_20{}.xlsx'.format(yr))\n",
    "    df = pd.concat([df, temp])\n",
    "\n",
    "# df에 연도 범주형 변수 만들어주기\n",
    "def date_to_year(date):\n",
    "    return date[:4]\n",
    "\n",
    "df['year'] = df['Date'].apply(lambda x: date_to_year(x))\n",
    "\n",
    "# 연도별로 한 열의 시리즈를 리스트로 만들고 하나의 스트링으로 이어 붙이기\n",
    "articles18 = df[df['year']=='2018']['Article'].tolist()\n",
    "articles18 = ' '.join(articles18)\n",
    "articles19 = df[df['year']=='2019']['Article'].tolist()\n",
    "articles19 = ' '.join(articles19)\n",
    "articles20 = df[df['year']=='2020']['Article'].tolist()\n",
    "articles20 = ' '.join(articles20)\n",
    "articles21 = df[df['year']=='2021']['Article'].tolist()\n",
    "articles21 = ' '.join(articles21)\n",
    "\n",
    "\n",
    "#==========================2018년만 일단..\n",
    "\n",
    "## 단어 정규화 및 태깅 후 단어 등장 빈도 카운팅\n",
    "\n",
    "# 단어 정규화 및 어근화 그리고 품사 태깅하기\n",
    "tokenizer = Okt()\n",
    "raw_pos_tagged = tokenizer.pos(articles18, norm = True, stem = True) #pos tagging\n",
    "raw_pos_tagged\n",
    "\n",
    "# 조사, 어미, 구두점, 외계어, SUFFIX, ALPHA, Determiner를 제외한 단어를 word_cleaned 리스트에 담기\n",
    "del_list = ['하다', '있다', '되다', '이다', '돼다', '않다', \n",
    "            '그렇다', '아니다', '이렇다', '그렇다', '어떻다',\n",
    "            '으로', '에서', '하고', '보다', '관련', '따르다',\n",
    "            '오다', '통해', '가다', '기자', '에는', '같다',\n",
    "            '이라고', '까지'] \n",
    "\n",
    "word_cleaned = []\n",
    "\n",
    "for word in raw_pos_tagged: # ('많다', 'Adjective')\n",
    "    if not word[1] in [\"josa\", \"Eomi\", \"Punctuation\", \"Foreign\", \"Suffix\", \"Alpha\", \"Determiner\"]:\n",
    "        if (len(word[0]) != 1) & (word[0] not in del_list):\n",
    "            word_cleaned.append(word[0])\n",
    "\n",
    "# word_cleaned에 담긴 단어들의 등장 횟수를 세는 dictionary 생성\n",
    "# word_dic = {}\n",
    "# for word in word_cleaned:\n",
    "#     if word not in word_dic:\n",
    "#         word_dic[word] = 1\n",
    "#     else:\n",
    "#         word_dic[word] += 1\n",
    "# word_dic\n",
    "\n",
    "# Counter를 통해서 단어들의 등장 횟수를 세는 dictionary 생성\n",
    "result = Counter(word_cleaned)\n",
    "word_dic = dict(result)\n",
    "\n",
    "# 내림차순으로 가장 많이 나온 단어 순으로 정리\n",
    "sorted_word_dic = sorted(word_dic.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_word_dic\n",
    "\n",
    "# 상위 50개만 살펴보고, 여기서 불용어 다시 추려보기\n",
    "for word, count in sorted_word_dic[:50]:\n",
    "    print(\"{0}({1})\".format(word, count), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46720ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 등장 빈도 시각화 (nltk의 .Text()함수)\n",
    "font_name = matplotlib.font_manager.FontProperties(fname=\"C:/Windows/Fonts/gulim.ttc\").get_name() # NanumGothic.otf\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "word_counted = nltk.Text(word_cleaned) \n",
    "plt.figure(figsize=(15, 7)) # plot 영역(그래프 영역)의 크기를 지정합니다.\n",
    "word_counted.plot(50) # \"plot\" the graph, 상위 50개 단어를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 막대 그래프 시각화\n",
    "word_frequency = nltk.FreqDist(word_cleaned) # Frequency Distribution - dict 형태\n",
    "df18 = pd.DataFrame(list(word_frequency.values()), word_frequency.keys())\n",
    "result = df18.sort_values([0], ascending=False) # 빈도를 내림차순 정렬\n",
    "result = result[:50] # 상위 50개만 시각화\n",
    "result.plot(kind='bar', legend=False, figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eadcb4",
   "metadata": {},
   "source": [
    "## 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633421fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b7c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fdb645",
   "metadata": {},
   "source": [
    "데이터 전처리단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e212b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_year(date):\n",
    "    return date[:4]\n",
    "\n",
    "df['year'] = df['Date'].apply(lambda x: date_to_year(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연도별로 한 열의 시리즈를 리스트로 만들고 하나의 스트링으로 이어 붙이기\n",
    "articles18 = df[df['year']=='2018']['Article'].tolist()\n",
    "articles18 = ' '.join(articles18)\n",
    "articles19 = df[df['year']=='2019']['Article'].tolist()\n",
    "articles19 = ' '.join(articles19)\n",
    "articles20 = df[df['year']=='2020']['Article'].tolist()\n",
    "articles20 = ' '.join(articles20)\n",
    "articles21 = df[df['year']=='2021']['Article'].tolist()\n",
    "articles21 = ' '.join(articles21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29477b07",
   "metadata": {},
   "source": [
    "2018년만 가지고 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 정규화 및 태깅 후 단어 등장 빈도 카운팅\n",
    "\n",
    "# 단어 정규화 및 어근화 그리고 품사 태깅하기\n",
    "tokenizer = Okt()\n",
    "raw_pos_tagged = tokenizer.pos(articles18, norm = True, stem = True) #pos tagging\n",
    "raw_pos_tagged\n",
    "\n",
    "# 조사, 어미, 구두점, 외계어, SUFFIX, ALPHA, Determiner를 제외한 단어를 word_cleaned 리스트에 담기\n",
    "del_list = ['하다', '있다', '되다', '이다', '돼다', '않다', \n",
    "            '그렇다', '아니다', '이렇다', '그렇다', '어떻다',\n",
    "            '으로', '에서', '하고', '보다', '관련', '따르다',\n",
    "            '오다', '통해', '가다', '기자', '에는', '같다',\n",
    "            '이라고', '까지'] \n",
    "\n",
    "word_cleaned = []\n",
    "\n",
    "for word in raw_pos_tagged: # ('많다', 'Adjective')\n",
    "    if not word[1] in [\"josa\", \"Eomi\", \"Punctuation\", \"Foreign\", \"Suffix\", \"Alpha\", \"Determiner\"]:\n",
    "        if (len(word[0]) != 1) & (word[0] not in del_list):\n",
    "            word_cleaned.append(word[0])\n",
    "\n",
    "# word_cleaned에 담긴 단어들의 등장 횟수를 세는 dictionary 생성\n",
    "# word_dic = {}\n",
    "# for word in word_cleaned:\n",
    "#     if word not in word_dic:\n",
    "#         word_dic[word] = 1\n",
    "#     else:\n",
    "#         word_dic[word] += 1\n",
    "# word_dic\n",
    "\n",
    "# Counter를 통해서 단어들의 등장 횟수를 세는 dictionary 생성\n",
    "from collections import Counter\n",
    "\n",
    "result = Counter(word_cleaned)\n",
    "word_dic = dict(result)\n",
    "word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478899ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내림차순으로 가장 많이 나온 단어 순으로 정리\n",
    "sorted_word_dic = sorted(word_dic.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_word_dic\n",
    "\n",
    "# 상위 50개만 살펴보고, 여기서 불용어 다시 추려보기\n",
    "for word, count in sorted_word_dic[:50]:\n",
    "    print(\"{0}({1})\".format(word, count), end = \" \")\n",
    "\n",
    "\n",
    "# 에서 하고 보다 관련 따르다 오다 통해 가다 기자 에는 같다 이라고 까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 등장 빈도 시각화 (nltk의 .Text()함수)\n",
    "font_name = matplotlib.font_manager.FontProperties(fname=\"C:/Windows/Fonts/gulim.ttc\").get_name() # NanumGothic.otf\n",
    "matplotlib.rc('font', family=font_name)\n",
    "\n",
    "word_counted = nltk.Text(word_cleaned) \n",
    "plt.figure(figsize=(15, 7)) # plot 영역(그래프 영역)의 크기를 지정합니다.\n",
    "word_counted.plot(50) # \"plot\" the graph, 상위 50개 단어를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 막대 그래프 시각화\n",
    "word_frequency = nltk.FreqDist(word_cleaned) # Frequency Distribution - dict 형태\n",
    "df18 = pd.DataFrame(list(word_frequency.values()), word_frequency.keys())\n",
    "result = df18.sort_values([0], ascending=False) # 빈도를 내림차순 정렬\n",
    "result = result[:50] # 상위 50개만 시각화\n",
    "result.plot(kind='bar', legend=False, figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ac149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 클라우드\n",
    "\n",
    "# WordCloud 객체를 생성하기\n",
    "\n",
    "# word_cloud = WordCloud().generate(text) : dict 가 아닌 줄글 텍스트 원문으로부터 워드클라우드를 만들 수 있습니다.\n",
    "word_cloud = WordCloud(font_path=\"C:/Windows/Fonts/gulim.ttc\",\n",
    "                       width=2000, height=1000, # 이 부분을 수정하시면 실제 워드클라우드의 크기가 바뀝니다 (해상도가 바뀝니다)\n",
    "                       # prefer_horizontal= 1.0, # 이 부분의 주석을 해제하시면 단어들이 가로로만 그려지게 됩니다. (0~1)\n",
    "                       background_color='black')\n",
    "\n",
    "word_cloud.generate_from_frequencies(word_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
