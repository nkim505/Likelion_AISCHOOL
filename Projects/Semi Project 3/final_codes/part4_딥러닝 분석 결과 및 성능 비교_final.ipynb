{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e812dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "\n",
    "# Feature Selection and Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, StandardScaler, MinMaxScaler,OrdinalEncoder\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "from sklearn import model_selection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Managing Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot the Figures Inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9e7917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv('./pre_credit_df(fill_groupby).csv', index_col=0) # 엑셀 파일 읽기\n",
    "data_df = pd.read_csv('./pre_credit_df(fill_groupby)_final.csv', index_col=0) # 엑셀 파일 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd3cd9",
   "metadata": {},
   "source": [
    "### Target, Feature 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb7dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_Y = data_df['credit']\n",
    "credit_X = data_df.drop(['credit'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190f426",
   "metadata": {},
   "source": [
    "### Pipeline 활용한 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218aace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_processing(x_train, x_test):\n",
    "    #numeric, categorical feature 정리\n",
    "    numeric_features = ['income_total','DAYS_EMPLOYED','family_size','begin_month','Age','cards']\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    categorical_features = ['child_num','income_type','edu_type','family_type','house_type','occyp_type','family_category','occyp_category']\n",
    "    categorical_transformer = OneHotEncoder()\n",
    "\n",
    "    pass_through = ['gender','car','reality','work_phone','phone','email','dup']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[ # List of (name, transformer, column(s))\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('pass_through','passthrough',pass_through)])\n",
    "\n",
    "    preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)]) # preprocessing-only\n",
    "\n",
    "    preprocessor_pipe.fit(x_train)\n",
    "    x_train_transformed = preprocessor_pipe.transform(x_train)\n",
    "    x_test_transformed = preprocessor_pipe.transform(x_test)\n",
    "    \n",
    "    return x_train_transformed, x_test_transformed, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19d2e0",
   "metadata": {},
   "source": [
    "### 인코딩한 Dataframe 열 이름 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58625a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(column_transformer):\n",
    "\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624df9f5",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da615ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, utils\n",
    "from tensorflow.keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4842659",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "num_folds= 5 # test_size = 0.2\n",
    "str_kf = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead1166",
   "metadata": {},
   "source": [
    "##### num_hidden_layer = 4\n",
    "##### optimizer = Adam\n",
    "##### initializer = he_normal\n",
    "##### BN 적용\n",
    "##### DROPOUT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b58e59b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.8135190010070801; categorical_accuracy of 68.11566948890686%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.8228822946548462; categorical_accuracy of 67.22117066383362%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.8188735842704773; categorical_accuracy of 68.07183623313904%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.8155917525291443; categorical_accuracy of 68.63893866539001%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.8386233448982239; categorical_accuracy of 67.29678511619568%\n"
     ]
    }
   ],
   "source": [
    "logloss_history = []\n",
    "accuracy_history = []\n",
    "fold_no = 1\n",
    "for train_index, test_index in str_kf.split(credit_X, credit_Y):\n",
    "    X_train, X_test = credit_X.loc[train_index], credit_X.loc[test_index]\n",
    "    y_train, y_test = credit_Y.loc[train_index], credit_Y.loc[test_index]\n",
    "    \n",
    "    x_train_transformed , x_test_transformed, preprocessor = pipe_processing(X_train, X_test)\n",
    "    new_col_names = get_feature_names(preprocessor)\n",
    "    x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "    x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "    train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "    test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "    \n",
    "    model = models.Sequential() \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization()) # BN은 적용하려면 매 레이어마다 해주는 것이 좋다.\n",
    "    model.add(layers.Activation('elu')) # layers.ELU or layers.LeakyReLU\n",
    "\n",
    "    model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu')) # layers.ELU or layers.LeakyReLU\n",
    "\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(),# 함수에 인자로 learning Rate 적용 가능 \n",
    "                  loss=losses.categorical_crossentropy, \n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train_transformed, train_label, batch_size=100, epochs=30, verbose = 0) \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_test_transformed, test_label, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    accuracy_history.append(scores[1] * 100)\n",
    "    logloss_history.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5140da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 분할의 loss 기록 : [0.8135190010070801, 0.8228822946548462, 0.8188735842704773, 0.8155917525291443, 0.8386233448982239]\n",
      "각 분할의 정확도 기록 : [68.11566948890686, 67.22117066383362, 68.07183623313904, 68.63893866539001, 67.29678511619568]\n",
      "평균 loss : 0.8218979954719543\n",
      "평균 정확도 : 67.86888003349304\n"
     ]
    }
   ],
   "source": [
    "print(\"각 분할의 loss 기록 :\", logloss_history)    \n",
    "print(\"각 분할의 정확도 기록 :\", accuracy_history)\n",
    "print(\"평균 loss :\", np.mean(logloss_history))\n",
    "print(\"평균 정확도 :\", np.mean(accuracy_history)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54097e63",
   "metadata": {},
   "source": [
    "##### num_hidden_layer = 4\n",
    "##### optimizer = RMSprop\n",
    "##### initializer = he_normal\n",
    "##### BN 적용\n",
    "##### DROPOUT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32575f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.8207738995552063; categorical_accuracy of 67.88886785507202%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.8187645077705383; categorical_accuracy of 68.62003803253174%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.84190434217453; categorical_accuracy of 68.16635131835938%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.8140873312950134; categorical_accuracy of 68.80907416343689%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.8331273198127747; categorical_accuracy of 67.58034229278564%\n"
     ]
    }
   ],
   "source": [
    "logloss_history = []\n",
    "accuracy_history = []\n",
    "fold_no = 1\n",
    "for train_index, test_index in str_kf.split(credit_X, credit_Y):\n",
    "    X_train, X_test = credit_X.loc[train_index], credit_X.loc[test_index]\n",
    "    y_train, y_test = credit_Y.loc[train_index], credit_Y.loc[test_index]\n",
    "    \n",
    "    x_train_transformed , x_test_transformed, preprocessor = pipe_processing(X_train, X_test)\n",
    "#     new_col_names = get_feature_names(preprocessor)\n",
    "#     x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "#     x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "    train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "    test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "    \n",
    "    model = models.Sequential() \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization()) # BN은 적용하려면 매 레이어마다 해주는 것이 좋다.\n",
    "    model.add(layers.Activation('elu')) # layers.ELU or layers.LeakyReLU\n",
    "\n",
    "    model.add(layers.Dense(units=512, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu')) # layers.ELU or layers.LeakyReLU\n",
    "\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=256, activation=None, kernel_initializer=initializers.he_normal())) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('elu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    model.compile(optimizer=optimizers.RMSprop(),# 함수에 인자로 learning Rate 적용 가능 \n",
    "                  loss=losses.categorical_crossentropy, \n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train_transformed, train_label, batch_size=100, epochs=30, verbose = 0) \n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_test_transformed, test_label, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    accuracy_history.append(scores[1] * 100)\n",
    "    logloss_history.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48a6da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 분할의 loss 기록 : []\n",
      "각 분할의 정확도 기록 : []\n",
      "평균 loss : nan\n",
      "평균 정확도 : nan\n"
     ]
    }
   ],
   "source": [
    "print(\"각 분할의 loss 기록 :\", logloss_history)    \n",
    "print(\"각 분할의 정확도 기록 :\", accuracy_history)\n",
    "print(\"평균 loss :\", np.mean(logloss_history))\n",
    "print(\"평균 정확도 :\", np.mean(accuracy_history)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05dcf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9008ea",
   "metadata": {},
   "source": [
    "### HPO(BayesianOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcea4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a3c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build the hyper-model\n",
    "# Available HyperParameter search spaces (https://j.mp/2IXPzh7) : Int, Float, Boolean, Choice, Fixed\n",
    "\n",
    "def build_hyper_model(hp):\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Flatten())\n",
    "        \n",
    "    # Tune the number of hidden layer (Choose an optimal value between 1~3)\n",
    "    for layer_num in range(hp.Int('num_layers', min_value=1, max_value=3)): \n",
    "        # Tune the number of perceptrons in a dense layer (Choose an optimal value between 32~512) \n",
    "        hp_units = hp.Int('units_' + str(layer_num), min_value=32, max_value=512, step=32) # 32:512 & step 32, all parameter names should be unique (we name the inner parameters 'units_' + str(i))\n",
    "        hp_activations = hp.Choice('activation_' + str(layer_num), values=['relu', 'elu'])\n",
    "        model.add(layers.Dense(units = hp_units, activation = hp_activations))\n",
    "\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    # Tune the learning rate for the optimizer (Choose an optimal value from 0.1, 0.01, or 0.001)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3]) \n",
    "    \n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = losses.categorical_crossentropy, \n",
    "                metrics=[metrics.categorical_crossentropy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d9dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project test_prac_dir\\Credit_hyper_1(final)\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from test_prac_dir\\Credit_hyper_1(final)\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(build_hyper_model,\n",
    "                                objective = kt.Objective('val_loss','min'), # Hyper-params tuning을 위한 목적함수 설정 (metric to minimize or maximize)\n",
    "                                max_trials = 30, # 서로 다른 Hyper-params 조합으로 시도할 총 Trial 횟수 설정\n",
    "                                directory = 'test_prac_dir', # Path to the working directory\n",
    "                                project_name = 'Credit_hyper_1(final)') # Name to use as directory name for files saved by this Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c542caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(credit_X, credit_Y, test_size=0.2, random_state=0)\n",
    "x_train_transformed , x_test_transformed, preprocessor = pipe_processing(x_train, x_test)\n",
    "new_col_names = get_feature_names(preprocessor)\n",
    "x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c653e237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train_transformed, train_label, epochs=30, validation_data = (x_test_transformed, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11372964",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in test_prac_dir\\Credit_hyper_1(final)\n",
      "Showing 3 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001C7E8EB6310>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 512\n",
      "activation_0: relu\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: elu\n",
      "units_2: 512\n",
      "activation_2: elu\n",
      "Score: 0.778764545917511\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 320\n",
      "activation_0: relu\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: relu\n",
      "units_2: 512\n",
      "activation_2: elu\n",
      "Score: 0.7788336277008057\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units_0: 224\n",
      "activation_0: relu\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: elu\n",
      "units_2: 512\n",
      "activation_2: relu\n",
      "Score: 0.7807657718658447\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2dae9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance rank : 0\n",
      "{'num_layers': 2, 'units_0': 512, 'activation_0': 'relu', 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'elu', 'units_2': 512, 'activation_2': 'elu'}\n",
      "\n",
      "Model performance rank : 1\n",
      "{'num_layers': 1, 'units_0': 320, 'activation_0': 'relu', 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'relu', 'units_2': 512, 'activation_2': 'elu'}\n",
      "\n",
      "Model performance rank : 2\n",
      "{'num_layers': 1, 'units_0': 224, 'activation_0': 'relu', 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'elu', 'units_2': 512, 'activation_2': 'relu'}\n",
      "\n",
      "\n",
      "The hyperparameter search is complete. \n",
      "* Optimal # of layers : 2\n",
      "* Optimal value of the learning-rate : 0.001\n",
      "Layer 0 - # of Perceptrons : 512\n",
      "Layer 0 - Applied activation function : relu\n",
      "Layer 1 - # of Perceptrons : 512\n",
      "Layer 1 - Applied activation function : elu\n"
     ]
    }
   ],
   "source": [
    "# Check top-3 trials' hyper-params\n",
    "\n",
    "top3_models = tuner.get_best_hyperparameters(num_trials=3)\n",
    "# print(tuner.get_best_hyperparameters(num_trials=3)[0].space) # 특정 Trial의 Search-space 를 확인할 수 있음\n",
    "# print(tuner.get_best_hyperparameters(num_trials=3)[0].values) # 특정 Trial에 적용된 Hyper-params를 확인할 수 있음\n",
    "\n",
    "for idx, model in enumerate(top3_models):\n",
    "    print('Model performance rank :', idx)\n",
    "    print(model.values)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Check the best trial's hyper-params\n",
    "\n",
    "best_hps = top3_models[0]\n",
    "\n",
    "print(\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "* Optimal # of layers : {}\n",
    "* Optimal value of the learning-rate : {}\"\"\".format(best_hps.get('num_layers'), best_hps.get('learning_rate')))\n",
    "\n",
    "for layer_num in range(best_hps.get('num_layers')):\n",
    "    print('Layer {} - # of Perceptrons :'.format(layer_num), best_hps.get('units_' + str(layer_num)))\n",
    "    print('Layer {} - Applied activation function :'.format(layer_num), best_hps.get('activation_' + str(layer_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c452890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b51ebfa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.8496361374855042; categorical_crossentropy of 84.96361374855042%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.8090305328369141; categorical_crossentropy of 80.9030532836914%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.8262324929237366; categorical_crossentropy of 82.62324929237366%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.8400170803070068; categorical_crossentropy of 84.00170803070068%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.839669406414032; categorical_crossentropy of 83.9669406414032%\n",
      "각 분할의 loss 기록 : [0.8496361374855042, 0.8090305328369141, 0.8262324929237366, 0.8400170803070068, 0.839669406414032]\n",
      "각 분할의 정확도 기록 : [84.96361374855042, 80.9030532836914, 82.62324929237366, 84.00170803070068, 83.9669406414032]\n",
      "평균 loss : 0.8329171299934387\n",
      "평균 정확도 : 83.29171299934387\n"
     ]
    }
   ],
   "source": [
    "logloss_history = []\n",
    "fold_no = 1\n",
    "for train_index, test_index in str_kf.split(credit_X, credit_Y):\n",
    "    X_train, X_test = credit_X.loc[train_index], credit_X.loc[test_index]\n",
    "    y_train, y_test = credit_Y.loc[train_index], credit_Y.loc[test_index]\n",
    "    \n",
    "    x_train_transformed , x_test_transformed, preprocessor = pipe_processing(X_train, X_test)\n",
    "    new_col_names = get_feature_names(preprocessor)\n",
    "    x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "    x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "    train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "    test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train_transformed, train_label, epochs=30, verbose = 0,validation_data = (x_test_transformed, test_label))\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_test_transformed, test_label, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]};')\n",
    "    logloss_history.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(\"각 분할의 loss 기록 :\", logloss_history)    \n",
    "print(\"평균 loss :\", np.mean(logloss_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3332c",
   "metadata": {},
   "source": [
    "### BN 적용한 HPO(BayesianOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cd05b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build the hyper-model\n",
    "# Available HyperParameter search spaces (https://j.mp/2IXPzh7) : Int, Float, Boolean, Choice, Fixed\n",
    "\n",
    "def build_hyper_model(hp):\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Flatten())\n",
    "        \n",
    "    # Tune the number of hidden layer (Choose an optimal value between 1~3)\n",
    "    for layer_num in range(hp.Int('num_layers', min_value=1, max_value=3)): \n",
    "        # Tune the number of perceptrons in a dense layer (Choose an optimal value between 32~512) \n",
    "        hp_units = hp.Int('units_' + str(layer_num), min_value=32, max_value=512, step=32) # 32:512 & step 32, all parameter names should be unique (we name the inner parameters 'units_' + str(i))\n",
    "        hp_activations = hp.Choice('activation_' + str(layer_num), values=['relu', 'elu'])\n",
    "        hp_dropout = hp.Float('dropout_', min_value=0.0, max_value=0.5, step=0.05)\n",
    "        model.add(layers.Dense(units = hp_units, activation = None))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(hp_activations))\n",
    "        model.add(layers.Dropout(hp_dropout))\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    # Tune the learning rate for the optimizer (Choose an optimal value from 0.01, 0.001, or 0.0001)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3]) \n",
    "    \n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = losses.categorical_crossentropy, \n",
    "                metrics=[metrics.categorical_crossentropy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa376ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project test_prac_dir\\Credit_hyper_2(final)\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from test_prac_dir\\Credit_hyper_2(final)\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(build_hyper_model,\n",
    "                                objective = kt.Objective('val_loss','min'), # Hyper-params tuning을 위한 목적함수 설정 (metric to minimize or maximize)\n",
    "                                max_trials = 30, # 서로 다른 Hyper-params 조합으로 시도할 총 Trial 횟수 설정\n",
    "                                directory = 'test_prac_dir', # Path to the working directory\n",
    "                                project_name = 'Credit_hyper_2(final)') # Name to use as directory name for files saved by this Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a4ce310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(credit_X, credit_Y, test_size=0.2, random_state=0)\n",
    "x_train_transformed , x_test_transformed, preprocessor = pipe_processing(x_train, x_test)\n",
    "new_col_names = get_feature_names(preprocessor)\n",
    "x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "tuner.search(x_train_transformed, train_label, epochs=30, validation_data = (x_test_transformed, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d594887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in test_prac_dir\\Credit_hyper_2(final)\n",
      "Showing 3 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001C7E9E6CDF0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation_0: elu\n",
      "dropout_: 0.2\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: relu\n",
      "units_2: 192\n",
      "activation_2: elu\n",
      "Score: 0.7753281593322754\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation_0: elu\n",
      "dropout_: 0.1\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: relu\n",
      "units_2: 32\n",
      "activation_2: elu\n",
      "Score: 0.7769176959991455\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation_0: elu\n",
      "dropout_: 0.1\n",
      "learning_rate: 0.001\n",
      "units_1: 512\n",
      "activation_1: relu\n",
      "units_2: 32\n",
      "activation_2: elu\n",
      "Score: 0.7777508497238159\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84d99735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance rank : 0\n",
      "{'num_layers': 3, 'units_0': 512, 'activation_0': 'elu', 'dropout_': 0.2, 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'relu', 'units_2': 192, 'activation_2': 'elu'}\n",
      "\n",
      "Model performance rank : 1\n",
      "{'num_layers': 3, 'units_0': 512, 'activation_0': 'elu', 'dropout_': 0.1, 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'relu', 'units_2': 32, 'activation_2': 'elu'}\n",
      "\n",
      "Model performance rank : 2\n",
      "{'num_layers': 3, 'units_0': 512, 'activation_0': 'elu', 'dropout_': 0.1, 'learning_rate': 0.001, 'units_1': 512, 'activation_1': 'relu', 'units_2': 32, 'activation_2': 'elu'}\n",
      "\n",
      "\n",
      "The hyperparameter search is complete. \n",
      "* Optimal # of layers : 3\n",
      "* Optimal value of the learning-rate : 0.001\n",
      "Layer 0 - # of Perceptrons : 512\n",
      "Layer 0 - Applied activation function : elu\n",
      "Layer 1 - # of Perceptrons : 512\n",
      "Layer 1 - Applied activation function : relu\n",
      "Layer 2 - # of Perceptrons : 192\n",
      "Layer 2 - Applied activation function : elu\n"
     ]
    }
   ],
   "source": [
    "# Check top-3 trials' hyper-params\n",
    "\n",
    "top3_models = tuner.get_best_hyperparameters(num_trials=3)\n",
    "# print(tuner.get_best_hyperparameters(num_trials=3)[0].space) # 특정 Trial의 Search-space 를 확인할 수 있음\n",
    "# print(tuner.get_best_hyperparameters(num_trials=3)[0].values) # 특정 Trial에 적용된 Hyper-params를 확인할 수 있음\n",
    "\n",
    "for idx, model in enumerate(top3_models):\n",
    "    print('Model performance rank :', idx)\n",
    "    print(model.values)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Check the best trial's hyper-params\n",
    "\n",
    "best_hps = top3_models[0]\n",
    "\n",
    "print(\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "* Optimal # of layers : {}\n",
    "* Optimal value of the learning-rate : {}\"\"\".format(best_hps.get('num_layers'), best_hps.get('learning_rate')))\n",
    "\n",
    "for layer_num in range(best_hps.get('num_layers')):\n",
    "    print('Layer {} - # of Perceptrons :'.format(layer_num), best_hps.get('units_' + str(layer_num)))\n",
    "    print('Layer {} - Applied activation function :'.format(layer_num), best_hps.get('activation_' + str(layer_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28fe0823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.7921059727668762;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.8007999062538147;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.7948606014251709;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.794422447681427;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.8013486266136169;\n",
      "각 분할의 loss 기록 : [0.7921059727668762, 0.8007999062538147, 0.7948606014251709, 0.794422447681427, 0.8013486266136169]\n",
      "평균 loss : 0.7967075109481812\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "logloss_history = []\n",
    "fold_no = 1\n",
    "for train_index, test_index in str_kf.split(credit_X, credit_Y):\n",
    "    X_train, X_test = credit_X.loc[train_index], credit_X.loc[test_index]\n",
    "    y_train, y_test = credit_Y.loc[train_index], credit_Y.loc[test_index]\n",
    "    \n",
    "    x_train_transformed , x_test_transformed, preprocessor = pipe_processing(X_train, X_test)\n",
    "    new_col_names = get_feature_names(preprocessor)\n",
    "    x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "    x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "    train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "    test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train_transformed, train_label, epochs=30, verbose = 0,validation_data = (x_test_transformed, test_label))\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_test_transformed, test_label, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]};')\n",
    "    logloss_history.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(\"각 분할의 loss 기록 :\", logloss_history)    \n",
    "print(\"평균 loss :\", np.mean(logloss_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c10ca",
   "metadata": {},
   "source": [
    "## Keras tuner : Hyperband 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e6531c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import utils,models, layers, activations, initializers, losses, optimizers, metrics\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Hyperband \n",
    "from kerastuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d1485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(data_df, target):\n",
    "    #자동으로 num과 cat 변수 갈라서 df 생성\n",
    "    data_df_cat = data_df.select_dtypes(include=np.object)\n",
    "    data_df_num = data_df.select_dtypes(exclude=np.object)\n",
    "\n",
    "    # category df 만들어주기\n",
    "    data_df_cat = pd.concat([data_df_cat, data_df[['occyp_category', 'child_num']]], axis=1)\n",
    "    \n",
    "    # binary df 만들어주기\n",
    "    data_df_bi = data_df[['gender','car','reality','work_phone','phone','email','dup']]\n",
    "\n",
    "    # cat인데 num df에 들어간 변수 num df에서 drop해주기\n",
    "    data_df_num = data_df_num.drop(columns=['gender','car','reality','child_num','work_phone','phone','email','dup','occyp_category'])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(data_df,\n",
    "                                                                    target,\n",
    "                                                                   test_size = 0.2,\n",
    "                                                                   random_state=0)\n",
    "\n",
    "    binary_features = data_df_bi.columns\n",
    "    \n",
    "    numeric_features = data_df_num.columns\n",
    "    numeric_transformer = StandardScaler() # cf) RobustScaler\n",
    "\n",
    "    categorical_features = data_df_cat.columns\n",
    "    categorical_transformer = OneHotEncoder(categories='auto', handle_unknown='ignore') # categories='auto' : just for ignoring warning messages\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[ # List of (name, transformer, column(s))\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "             ('bi','passthrough',binary_features)])\n",
    "\n",
    "    preprocessor_pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    preprocessor_pipe.fit(x_train)\n",
    "\n",
    "    x_train_transformed = preprocessor_pipe.transform(x_train)\n",
    "    x_test_transformed = preprocessor_pipe.transform(x_test)\n",
    "\n",
    "    return x_train_transformed, x_test_transformed, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae7c98",
   "metadata": {},
   "source": [
    "### hyperband model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3406d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "     # Tune the number of hidden layer (Choose an optimal value between 1~3)\n",
    "    for layer_num in range(hp.Int('num_layers', min_value=1, max_value=3)): \n",
    "        # Tune the number of perceptrons in a dense layer (Choose an optimal value between 32~512) \n",
    "        hp_units = hp.Int('units_' + str(layer_num), min_value=32, max_value=512, step=32) # 32:512 & step 32, all parameter names should be unique (we name the inner parameters 'units_' + str(i))\n",
    "        hp_activations = hp.Choice('activation_' + str(layer_num), values=['relu', 'elu'])\n",
    "        model.add(layers.Dense(units = hp_units, activation = hp_activations))\n",
    "\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    # Tune the learning rate for the optimizer (Choose an optimal value from 0.01, 0.001, or 0.0001)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "    \n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = losses.categorical_crossentropy, \n",
    "                metrics=[metrics.categorical_crossentropy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468865c3",
   "metadata": {},
   "source": [
    "### BN, Dropout 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8171e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "     # Tune the number of hidden layer (Choose an optimal value between 1~3)\n",
    "    for layer_num in range(hp.Int('num_layers', min_value=1, max_value=3)): \n",
    "        # Tune the number of perceptrons in a dense layer (Choose an optimal value between 32~512) \n",
    "        hp_units = hp.Int('units_' + str(layer_num), min_value=32, max_value=512, step=32) # 32:512 & step 32, all parameter names should be unique (we name the inner parameters 'units_' + str(i))\n",
    "        hp_activations = hp.Choice('activation_' + str(layer_num), values=['relu', 'elu'])\n",
    "        hp_dropout = hp.Float('dropout_', min_value=0.0, max_value=0.5, step=0.05)\n",
    "        model.add(layers.Dense(units = hp_units, activation = None))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(hp_activations))\n",
    "        model.add(layers.Dropout(hp_dropout))\n",
    "\n",
    "    model.add(layers.Dense(units=3, activation='softmax')) # 0~2 \n",
    "\n",
    "    # Tune the learning rate for the optimizer (Choose an optimal value from 0.01, 0.001, or 0.0001)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-1, 1e-2, 1e-3]) \n",
    "    \n",
    "    model.compile(optimizer = optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = losses.categorical_crossentropy, \n",
    "                metrics=[metrics.categorical_crossentropy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67765fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('pre_credit_df(fill_groupby)_final.csv')\n",
    "credit_Y = data_df['credit']\n",
    "credit_X = data_df.drop(['credit'],axis=1)\n",
    "x_train_transformed, x_test_transformed, y_train, y_test = run_pipeline(credit_X, credit_Y)\n",
    "\n",
    "y_train = utils.to_categorical(y_train, 3)\n",
    "y_test = utils.to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "513ae832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperband\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = kt.Objective('val_loss','min'), \n",
    "                     max_epochs = 30,\n",
    "                     factor = 3,\n",
    "                     directory = 'test_prac_dir', # Path to the working directory\n",
    "                     project_name = 'Credit_hyperband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07a22431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train_transformed, y_train, epochs = 30, validation_data = (x_test_transformed, y_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa505de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "662/662 [==============================] - 4s 4ms/step - loss: 0.9168 - categorical_crossentropy: 0.9168 - val_loss: 0.8440 - val_categorical_crossentropy: 0.8440\n",
      "Epoch 2/15\n",
      "662/662 [==============================] - 2s 4ms/step - loss: 0.8448 - categorical_crossentropy: 0.8448 - val_loss: 0.8347 - val_categorical_crossentropy: 0.8347\n",
      "Epoch 3/15\n",
      "662/662 [==============================] - 2s 4ms/step - loss: 0.8298 - categorical_crossentropy: 0.8298 - val_loss: 0.8313 - val_categorical_crossentropy: 0.8313\n",
      "Epoch 4/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.8123 - categorical_crossentropy: 0.8123 - val_loss: 0.8187 - val_categorical_crossentropy: 0.8187\n",
      "Epoch 5/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7990 - categorical_crossentropy: 0.7990 - val_loss: 0.8183 - val_categorical_crossentropy: 0.8183\n",
      "Epoch 6/15\n",
      "662/662 [==============================] - 2s 4ms/step - loss: 0.7881 - categorical_crossentropy: 0.7881 - val_loss: 0.8200 - val_categorical_crossentropy: 0.8200\n",
      "Epoch 7/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7734 - categorical_crossentropy: 0.7734 - val_loss: 0.8184 - val_categorical_crossentropy: 0.8184\n",
      "Epoch 8/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7638 - categorical_crossentropy: 0.7638 - val_loss: 0.8176 - val_categorical_crossentropy: 0.8176\n",
      "Epoch 9/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7483 - categorical_crossentropy: 0.7483 - val_loss: 0.8105 - val_categorical_crossentropy: 0.8105\n",
      "Epoch 10/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7405 - categorical_crossentropy: 0.7405 - val_loss: 0.8153 - val_categorical_crossentropy: 0.8153\n",
      "Epoch 11/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7336 - categorical_crossentropy: 0.7336 - val_loss: 0.8167 - val_categorical_crossentropy: 0.8167\n",
      "Epoch 12/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7203 - categorical_crossentropy: 0.7203 - val_loss: 0.8239 - val_categorical_crossentropy: 0.8239\n",
      "Epoch 13/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7097 - categorical_crossentropy: 0.7097 - val_loss: 0.8292 - val_categorical_crossentropy: 0.8292\n",
      "Epoch 14/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.7001 - categorical_crossentropy: 0.7001 - val_loss: 0.8252 - val_categorical_crossentropy: 0.8252\n",
      "Epoch 15/15\n",
      "662/662 [==============================] - 2s 3ms/step - loss: 0.6934 - categorical_crossentropy: 0.6934 - val_loss: 0.8140 - val_categorical_crossentropy: 0.8140\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "hb_history = model.fit(x_train_transformed, y_train, epochs = 15, validation_data = (x_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3cd6d7d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperband Logloss :  0.8140285514455103\n"
     ]
    }
   ],
   "source": [
    "pred_result = model.predict_proba(x_test_transformed)\n",
    "pre_logloss = log_loss(y_test, pred_result)\n",
    "print('Hyperband Logloss : ',pre_logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc15954",
   "metadata": {},
   "source": [
    "### k-fold 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "78a35b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.8564336895942688;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.8435595035552979;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.8567010164260864;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.837535560131073;\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.8494362235069275;\n",
      "각 분할의 loss 기록 : [0.8564336895942688, 0.8435595035552979, 0.8567010164260864, 0.837535560131073, 0.8494362235069275]\n",
      "평균 loss : 0.8487331986427307\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "logloss_history = []\n",
    "fold_no = 1\n",
    "for train_index, test_index in str_kf.split(credit_X, credit_Y):\n",
    "    X_train, X_test = credit_X.loc[train_index], credit_X.loc[test_index]\n",
    "    y_train, y_test = credit_Y.loc[train_index], credit_Y.loc[test_index]\n",
    "    \n",
    "    x_train_transformed , x_test_transformed, preprocessor = pipe_processing(X_train, X_test)\n",
    "#     new_col_names = get_feature_names(preprocessor)\n",
    "#     x_train_transformed = pd.DataFrame(x_train_transformed,columns=new_col_names)\n",
    "#     x_test_transformed = pd.DataFrame(x_test_transformed,columns=new_col_names)\n",
    "    train_label = utils.to_categorical(y_train) # 0~2-> one-hot vector\n",
    "    test_label = utils.to_categorical(y_test) # 0~2 -> one-hot vector\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train_transformed, train_label, epochs=30, verbose = 0,validation_data = (x_test_transformed, test_label))\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_test_transformed, test_label, verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]};')\n",
    "    logloss_history.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(\"각 분할의 loss 기록 :\", logloss_history)    \n",
    "print(\"평균 loss :\", np.mean(logloss_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e82178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
